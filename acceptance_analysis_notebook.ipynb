{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ICLR 2026 Acceptance Analysis\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/leippold/HAI-Frontier/blob/main/acceptance_analysis_notebook.ipynb)\n\n## Analysis of AI Content Effects on Acceptance Decisions\n\n**ICLR 2026 decisions are now available!** This notebook analyzes whether AI-generated content in papers affects acceptance decisions.\n\n### Tests Implemented:\n\n#### Part A: AI Content ‚Üí Acceptance\n1. **AI ‚Üí Acceptance | Scores** - Probit/Logit regression testing if AI content predicts acceptance conditional on review scores\n2. **Threshold Discontinuity** - RDD-style analysis near the decision margin (tiebreaker effect)\n3. **Presentation Tier Analysis** - Ordered probit for tier assignment among accepted papers\n4. **Author Reputation √ó Acceptance** - Does reputation insulate from AI penalty?\n\n#### Part B: Echo Chamber Analysis (AI Reviews)\n5. **AI Reviews √ó AI Papers** - Do AI-assisted reviews rate AI papers differently?\n6. **Acceptance | AI Review Ratings** - Does acceptance depend on AI content conditional on AI reviewer scores?\n\n### Key Research Questions:\n- Does AI content predict acceptance **conditional on scores**? (discrimination beyond the score channel)\n- Is AI content used as a **tiebreaker** near the acceptance margin?\n- Are high-AI papers relegated to **lower presentation tiers**?\n- Does **author reputation** protect against the AI penalty?\n- Do **AI reviewers** systematically favor or penalize **AI papers**? (echo chamber effect)"
  },
  {
   "cell_type": "code",
   "source": "#@title 1. Load GitHub Token from Colab Secrets { display-mode: \"form\" }\n#@markdown Add your GitHub token to Colab Secrets (key icon in left sidebar) with name `GITHUB_TOKEN`\n\nfrom google.colab import userdata\n\ntry:\n    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n    print(\"‚úì GitHub token loaded from Colab Secrets\")\nexcept:\n    GITHUB_TOKEN = None\n    print(\"‚ö†Ô∏è  No GitHub token found - clone may fail for private repos\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title 2. Configuration { display-mode: \"form\" }\n\n#@markdown ### Data Location\n#@markdown Path to your data folder in Google Drive:\nDATA_PATH = \"/content/drive/MyDrive/HAI_Data\"  #@param {type:\"string\"}\n\n#@markdown ---\n#@markdown ### GitHub Configuration\nGITHUB_USER = \"leippold\"  #@param {type:\"string\"}\nREPO_NAME = \"HAI-Frontier\"  #@param {type:\"string\"}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title 3. Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\nprint(\"‚úì Google Drive mounted\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title 4. Clone Repository from GitHub\nimport os\n\n%cd /content\n\n# Clean up any existing clone\n!rm -rf /content/{REPO_NAME}\n\n# Clone the repo (with token if available)\nif GITHUB_TOKEN:\n    !git clone https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git\nelse:\n    !git clone https://github.com/{GITHUB_USER}/{REPO_NAME}.git\n\n# Change to repo directory\n%cd /content/{REPO_NAME}\n\nprint(f\"\\n‚úì Repository cloned to: /content/{REPO_NAME}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title 5. Install Dependencies & Setup Python Path\n!pip install -q pandas numpy matplotlib seaborn scipy statsmodels requests openreview-py\n\nimport sys\nimport os\nREPO_PATH = f\"/content/{REPO_NAME}\"\nsys.path.insert(0, f\"{REPO_PATH}/iclr_analysis\")\n\n# Create output directory\nos.makedirs('last_results', exist_ok=True)\n\nprint(\"‚úì Dependencies installed (including openreview-py)\")\nprint(f\"‚úì Python path configured\")\nprint(f\"‚úì Working directory: {os.getcwd()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import custom modules\nimport sys\nsys.path.insert(0, 'iclr_analysis')\n\nfrom src.openreview_api import (\n    fetch_iclr_decisions,\n    merge_acceptance_data,\n    load_or_fetch_decisions,\n    check_decision_availability,\n    extract_forum_id\n)\n\nfrom analysis.acceptance_analysis import (\n    run_acceptance_analysis,\n    ai_acceptance_probit,\n    threshold_discontinuity,\n    optimal_bandwidth_selection,\n    presentation_tier_analysis,\n    reputation_acceptance_interaction,\n    selection_bounds_analysis,\n    score_acceptance_residual,\n    prepare_acceptance_data,\n    ai_acceptance_by_category\n)\n\n# Echo chamber analysis for AI reviews\nfrom analysis.echo_chamber import (\n    run_echo_chamber_analysis,\n    compute_interaction_effect,\n    test_interaction_comprehensive\n)\n\nfrom src.data_loading import load_data, clean_ai_percentage, prepare_echo_chamber_data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "First, we load the existing submissions data and check if acceptance decisions are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submissions data\n",
    "submissions_path = 'data/iclr_submissions_enriched.csv'\n",
    "submissions_df = pd.read_csv(submissions_path)\n",
    "\n",
    "print(f\"Loaded {len(submissions_df):,} submissions\")\n",
    "print(f\"\\nColumns: {list(submissions_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if acceptance data already exists\n",
    "if 'accepted' in submissions_df.columns:\n",
    "    print(\"Acceptance data already present!\")\n",
    "    print(f\"Acceptance rate: {submissions_df['accepted'].mean():.1%}\")\n",
    "else:\n",
    "    print(\"No acceptance data found - will need to fetch from OpenReview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#@title 2. Fetch Acceptance Data from OpenReview\n\n#@markdown If acceptance decisions are not in the data, we fetch them from OpenReview.\n#@markdown \n#@markdown **Note**: This requires ICLR to have released decisions. Check availability first.\n#@markdown \n#@markdown ### Caching Strategy\n#@markdown Fetched acceptance data is cached in Google Drive (`DATA_PATH`) to avoid redundant API calls.\n#@markdown Set `USE_CACHE = True` to load from cache if available.\n\nUSE_CACHE = True  #@param {type:\"boolean\"}\nICLR_YEAR = 2026  #@param {type:\"integer\"}\n\nimport os\n\n# Cache file path in Google Drive\nACCEPTANCE_CACHE_FILE = f\"{DATA_PATH}/iclr{ICLR_YEAR}_acceptance_data.csv\"\n\nprint(f\"Cache file: {ACCEPTANCE_CACHE_FILE}\")\nprint(f\"Use cache: {USE_CACHE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Fetch ICLR 2026 Accepted Papers from OpenReview (with Caching)\nimport openreview\nimport os\n\nvenue_id = f'ICLR.cc/{ICLR_YEAR}/Conference'\naccepted_ids = set()\naccepted_titles = {}\n\n# Check if cached data exists in Google Drive\nif USE_CACHE and os.path.exists(ACCEPTANCE_CACHE_FILE):\n    print(f\"‚úì Loading cached acceptance data from Google Drive...\")\n    cached_df = pd.read_csv(ACCEPTANCE_CACHE_FILE)\n    accepted_ids = set(cached_df['forum_id'].dropna().tolist())\n    \n    # Reconstruct title mapping if available\n    if 'title' in cached_df.columns:\n        for _, row in cached_df.iterrows():\n            if pd.notna(row.get('title')):\n                accepted_titles[row['title'].lower().strip()] = row.get('forum_id', '')\n    \n    print(f\"‚úì Loaded {len(accepted_ids):,} accepted paper IDs from cache\")\n    print(f\"  With titles: {len(accepted_titles):,}\")\n    \nelse:\n    print(f\"Connecting to OpenReview API...\")\n    client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')\n\n    print(f\"Fetching accepted papers for {venue_id}...\")\n    accepted_papers = list(client.get_all_notes(content={'venueid': venue_id}))\n\n    print(f\"\\n‚úì Found {len(accepted_papers):,} accepted papers\")\n\n    # Extract accepted paper IDs and titles\n    accepted_data = []\n    for note in accepted_papers:\n        accepted_ids.add(note.id)\n        title = note.content.get('title', {}).get('value', '')\n        if title:\n            accepted_titles[title.lower().strip()] = note.id\n        accepted_data.append({\n            'forum_id': note.id,\n            'title': title,\n            'venue_id': venue_id\n        })\n\n    print(f\"  Unique IDs: {len(accepted_ids)}\")\n    print(f\"  With titles: {len(accepted_titles)}\")\n    \n    # Save to Google Drive cache\n    print(f\"\\nüìÅ Saving acceptance data to Google Drive cache...\")\n    accepted_cache_df = pd.DataFrame(accepted_data)\n    os.makedirs(os.path.dirname(ACCEPTANCE_CACHE_FILE), exist_ok=True)\n    accepted_cache_df.to_csv(ACCEPTANCE_CACHE_FILE, index=False)\n    print(f\"‚úì Saved to: {ACCEPTANCE_CACHE_FILE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Match Accepted Papers with AI Content Data\nfrom src.openreview_api import extract_forum_id\n\n# Extract forum IDs from our submissions data\nsubmissions_df['forum_id'] = submissions_df['openreview_url'].apply(extract_forum_id)\n\n# Match by forum_id first\nsubmissions_df['accepted'] = submissions_df['forum_id'].isin(accepted_ids).astype(int)\n\n# For papers without forum_id match, try title matching\nno_match = submissions_df['accepted'] == 0\nif no_match.any():\n    submissions_df['_title_norm'] = submissions_df['title'].str.lower().str.strip()\n    title_matches = submissions_df['_title_norm'].isin(accepted_titles.keys())\n    submissions_df.loc[title_matches & no_match, 'accepted'] = 1\n    submissions_df = submissions_df.drop(columns=['_title_norm'])\n\n# Create df_with_decisions for downstream analysis\ndf_with_decisions = submissions_df.copy()\n\n# Summary\nn_accepted = df_with_decisions['accepted'].sum()\nn_total = len(df_with_decisions)\nacceptance_rate = n_accepted / n_total\n\nprint(\"=\" * 60)\nprint(\"MATCHING RESULTS\")\nprint(\"=\" * 60)\nprint(f\"Total submissions in data: {n_total:,}\")\nprint(f\"Matched as accepted: {n_accepted:,}\")\nprint(f\"Acceptance rate: {acceptance_rate:.1%}\")\nprint(f\"\\nExpected from OpenReview: {len(accepted_papers):,} accepted\")\n\nif n_accepted != len(accepted_papers):\n    print(f\"\\n‚ö†Ô∏è  Difference: {len(accepted_papers) - n_accepted} papers\")\n    print(\"   (Some accepted papers may not be in our submission data)\")\n\nprint(f\"\\n‚úì 'accepted' column added to dataframe\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Verify Data Quality\nfrom src.data_loading import clean_ai_percentage\n\n# Clean AI percentage column\ndf_with_decisions = clean_ai_percentage(df_with_decisions)\n\nprint(\"=\" * 60)\nprint(\"DATA QUALITY CHECK\")\nprint(\"=\" * 60)\n\nprint(f\"\\nSample of accepted papers:\")\naccepted_sample = df_with_decisions[df_with_decisions['accepted'] == 1][['title', 'avg_rating', 'ai_percentage']].head(5)\nprint(accepted_sample.to_string())\n\nprint(f\"\\nSample of rejected papers:\")\nrejected_sample = df_with_decisions[df_with_decisions['accepted'] == 0][['title', 'avg_rating', 'ai_percentage']].head(5)\nprint(rejected_sample.to_string())\n\nprint(f\"\\nAI percentage distribution:\")\nprint(df_with_decisions['ai_percentage'].describe())\n\nprint(f\"\\nMissing values:\")\nprint(f\"  avg_rating: {df_with_decisions['avg_rating'].isna().sum()}\")\nprint(f\"  ai_percentage: {df_with_decisions['ai_percentage'].isna().sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "if 'accepted' in df_with_decisions.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nTotal papers: {len(df_with_decisions):,}\")\n",
    "    print(f\"Acceptance rate: {df_with_decisions['accepted'].mean():.1%}\")\n",
    "    print(f\"Mean AI percentage: {df_with_decisions['ai_percentage'].mean():.1f}%\")\n",
    "    print(f\"Mean avg_rating: {df_with_decisions['avg_rating'].mean():.2f}\")\n",
    "    \n",
    "    if 'tier_name' in df_with_decisions.columns:\n",
    "        print(f\"\\nTier distribution:\")\n",
    "        print(df_with_decisions['tier_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize acceptance by AI content and rating\n",
    "if 'accepted' in df_with_decisions.columns:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 1. Rating distribution by acceptance\n",
    "    ax1 = axes[0]\n",
    "    df_with_decisions.boxplot(column='avg_rating', by='accepted', ax=ax1)\n",
    "    ax1.set_xlabel('Accepted')\n",
    "    ax1.set_ylabel('Average Rating')\n",
    "    ax1.set_title('Rating by Acceptance')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # 2. AI content by acceptance\n",
    "    ax2 = axes[1]\n",
    "    df_with_decisions.boxplot(column='ai_percentage', by='accepted', ax=ax2)\n",
    "    ax2.set_xlabel('Accepted')\n",
    "    ax2.set_ylabel('AI Percentage')\n",
    "    ax2.set_title('AI Content by Acceptance')\n",
    "    \n",
    "    # 3. Acceptance rate by AI quartile\n",
    "    ax3 = axes[2]\n",
    "    df_temp = df_with_decisions.copy()\n",
    "    df_temp['ai_quartile'] = pd.qcut(df_temp['ai_percentage'], 4, labels=['Q1 (Low)', 'Q2', 'Q3', 'Q4 (High)'])\n",
    "    acc_by_quartile = df_temp.groupby('ai_quartile')['accepted'].mean()\n",
    "    acc_by_quartile.plot(kind='bar', ax=ax3, color='steelblue', edgecolor='black')\n",
    "    ax3.set_xlabel('AI Content Quartile')\n",
    "    ax3.set_ylabel('Acceptance Rate')\n",
    "    ax3.set_title('Acceptance Rate by AI Quartile')\n",
    "    ax3.tick_params(axis='x', rotation=0)\n",
    "    ax3.axhline(y=df_with_decisions['accepted'].mean(), color='red', linestyle='--', label='Overall')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Comprehensive Acceptance Analysis\n",
    "\n",
    "Run all statistical tests at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all acceptance analysis tests\n",
    "if 'accepted' in df_with_decisions.columns:\n",
    "    results = run_acceptance_analysis(\n",
    "        df_with_decisions,\n",
    "        controls=['first_author_h_index', 'international_collaboration'],  # Optional controls\n",
    "        run_all=True,\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    print(\"No acceptance data available. Please fetch decisions first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Individual Tests (Detailed Results)\n",
    "\n",
    "Run individual tests for more detailed examination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: AI ‚Üí Acceptance | Scores (Probit Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: AI ‚Üí Acceptance conditional on scores\n",
    "if 'accepted' in df_with_decisions.columns:\n",
    "    probit_results = ai_acceptance_probit(\n",
    "        df_with_decisions,\n",
    "        controls=['first_author_h_index'],\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unadjusted comparison by AI category\n",
    "if 'accepted' in df_with_decisions.columns:\n",
    "    category_results = ai_acceptance_by_category(df_with_decisions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Threshold Discontinuity (Tiebreaker Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Threshold discontinuity\n",
    "if 'accepted' in df_with_decisions.columns:\n",
    "    # First, let's look at the score distribution to find a good margin\n",
    "    print(\"Score distribution around decision threshold:\")\n",
    "    print(df_with_decisions['avg_rating'].describe())\n",
    "    \n",
    "    # Acceptance rate by score bin\n",
    "    df_temp = df_with_decisions.copy()\n",
    "    df_temp['score_bin'] = pd.cut(df_temp['avg_rating'], bins=20)\n",
    "    acc_by_score = df_temp.groupby('score_bin').agg({\n",
    "        'accepted': ['mean', 'count']\n",
    "    }).round(3)\n",
    "    print(\"\\nAcceptance rate by score bin:\")\n",
    "    print(acc_by_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run threshold discontinuity analysis\n",
    "if 'accepted' in df_with_decisions.columns:\n",
    "    # Find approximate decision threshold (where acceptance ~ 50%)\n",
    "    disc_results = threshold_discontinuity(\n",
    "        df_with_decisions,\n",
    "        margin_lower=4.8,  # Adjust based on your data\n",
    "        margin_upper=5.2,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandwidth sensitivity analysis\n",
    "if 'accepted' in df_with_decisions.columns:\n",
    "    bandwidth_results = optimal_bandwidth_selection(\n",
    "        df_with_decisions,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Presentation Tier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Presentation tier analysis (among accepted papers)\n",
    "if 'accepted' in df_with_decisions.columns and ('tier' in df_with_decisions.columns or 'tier_name' in df_with_decisions.columns):\n",
    "    tier_results = presentation_tier_analysis(\n",
    "        df_with_decisions,\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Tier data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Author Reputation Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Reputation √ó AI interaction\n",
    "if 'accepted' in df_with_decisions.columns and 'first_author_h_index' in df_with_decisions.columns:\n",
    "    # Check data availability\n",
    "    n_with_hindex = df_with_decisions['first_author_h_index'].notna().sum()\n",
    "    print(f\"Papers with h-index data: {n_with_hindex}\")\n",
    "    \n",
    "    if n_with_hindex >= 100:\n",
    "        rep_results = reputation_acceptance_interaction(\n",
    "            df_with_decisions,\n",
    "            reputation_col='first_author_h_index',\n",
    "            verbose=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"Insufficient h-index data for this analysis\")\n",
    "else:\n",
    "    print(\"Required columns not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: Selection Bounds Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Selection bounds\n",
    "if 'first_author_h_index' in df_with_decisions.columns:\n",
    "    bounds_results = selection_bounds_analysis(\n",
    "        df_with_decisions,\n",
    "        observed_col='first_author_h_index',\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score-Acceptance Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "if 'accepted' in df_with_decisions.columns:\n",
    "    residual_results = score_acceptance_residual(\n",
    "        df_with_decisions,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Part B: Echo Chamber Analysis (AI Reviews Rating AI Papers)\n\nThis section tests whether AI-assisted reviews systematically rate AI-written papers differently.\n\n**Key Question**: Do AI reviewers give higher/lower ratings to AI papers compared to human reviewers?\n\nThe interaction effect captures the \"echo chamber\" hypothesis:\n- Positive interaction ‚Üí AI reviewers favor AI papers\n- Negative interaction ‚Üí AI reviewers penalize AI papers",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Prepare data for echo chamber analysis\n# This requires review-level data with AI percentage for each review\n# If only paper-level avg_rating is available, we analyze AI content effects on ratings\n\nif 'accepted' in df_with_decisions.columns:\n    # Create AI paper indicator\n    df_echo = df_with_decisions.copy()\n    df_echo = clean_ai_percentage(df_echo)\n    \n    # Define AI paper threshold (papers with high AI content)\n    AI_PAPER_THRESHOLD = 75  # Papers with >=75% AI content are \"AI papers\"\n    HUMAN_PAPER_THRESHOLD = 25  # Papers with <=25% AI content are \"human papers\"\n    \n    df_echo['paper_type'] = pd.cut(\n        df_echo['ai_percentage'],\n        bins=[-1, HUMAN_PAPER_THRESHOLD, AI_PAPER_THRESHOLD, 101],\n        labels=['Human Paper', 'Mixed', 'AI Paper']\n    )\n    \n    print(\"Paper Type Distribution:\")\n    print(df_echo['paper_type'].value_counts())\n    print(f\"\\nAI Paper threshold: >= {AI_PAPER_THRESHOLD}%\")\n    print(f\"Human Paper threshold: <= {HUMAN_PAPER_THRESHOLD}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare acceptance rates: AI Papers vs Human Papers (with Effect Sizes)\nif 'accepted' in df_with_decisions.columns:\n    from scipy import stats\n    from src.stats_utils import cohens_d, hedges_g, cliffs_delta, bootstrap_diff_ci\n    \n    print(\"=\" * 70)\n    print(\"ACCEPTANCE RATES BY PAPER TYPE (with Effect Sizes)\")\n    print(\"=\" * 70)\n    \n    # Exclude mixed papers for clean comparison\n    df_clean = df_echo[df_echo['paper_type'].isin(['Human Paper', 'AI Paper'])]\n    \n    acc_by_type = df_clean.groupby('paper_type').agg({\n        'accepted': ['mean', 'sum', 'count'],\n        'avg_rating': ['mean', 'std']\n    }).round(3)\n    acc_by_type.columns = ['Acceptance Rate', 'N Accepted', 'N Total', 'Mean Rating', 'Std Rating']\n    print(acc_by_type)\n    \n    # Get acceptance arrays for each group\n    ai_papers = df_clean[df_clean['paper_type'] == 'AI Paper']['accepted'].values\n    human_papers = df_clean[df_clean['paper_type'] == 'Human Paper']['accepted'].values\n    \n    # Chi-square test for acceptance rates\n    contingency = pd.crosstab(df_clean['paper_type'], df_clean['accepted'])\n    chi2, p_chi2, dof, expected = stats.chi2_contingency(contingency)\n    \n    # Cram√©r's V (effect size for chi-square)\n    n = contingency.sum().sum()\n    min_dim = min(contingency.shape) - 1\n    cramers_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0\n    \n    print(f\"\\n*** Chi-square test for independence ***\")\n    print(f\"Chi2: {chi2:.3f}, p-value: {p_chi2:.4e}\")\n    print(f\"Cram√©r's V (effect size): {cramers_v:.4f}\")\n    print(f\"Significant difference: {'YES' if p_chi2 < 0.05 else 'NO'}\")\n    \n    # Effect sizes for the acceptance rate difference\n    print(f\"\\n*** Effect Sizes ***\")\n    \n    # Cohen's d\n    d = cohens_d(ai_papers, human_papers)\n    print(f\"Cohen's d: {d:.4f}\")\n    \n    # Hedges' g (bias-corrected)\n    g = hedges_g(ai_papers, human_papers)\n    print(f\"Hedges' g (bias-corrected): {g:.4f}\")\n    \n    # Cliff's delta (non-parametric)\n    delta = cliffs_delta(ai_papers, human_papers)\n    print(f\"Cliff's Delta: {delta:.4f}\")\n    \n    # Bootstrap confidence interval for the difference\n    boot_ci = bootstrap_diff_ci(ai_papers, human_papers, n_bootstrap=5000)\n    print(f\"\\n*** Bootstrap CI for Acceptance Rate Difference ***\")\n    print(f\"Difference (AI - Human): {boot_ci['diff']:.4f}\")\n    print(f\"95% CI: [{boot_ci['ci_lower']:.4f}, {boot_ci['ci_upper']:.4f}]\")\n    print(f\"Bootstrap SE: {boot_ci['se']:.4f}\")\n    \n    # Interpretation\n    print(f\"\\n*** Interpretation ***\")\n    if abs(d) < 0.2:\n        size_label = \"negligible\"\n    elif abs(d) < 0.5:\n        size_label = \"small\"\n    elif abs(d) < 0.8:\n        size_label = \"medium\"\n    else:\n        size_label = \"large\"\n    print(f\"Effect size is {size_label} (|d| = {abs(d):.3f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Acceptance Conditional on Review Ratings\n\nDoes AI content predict acceptance **beyond** what review scores would predict?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Acceptance conditional on rating - Does AI content matter beyond scores?\nif 'accepted' in df_with_decisions.columns:\n    from scipy import stats\n    import statsmodels.api as sm\n    from statsmodels.discrete.discrete_model import Probit, Logit\n    \n    print(\"=\" * 70)\n    print(\"ACCEPTANCE CONDITIONAL ON REVIEW RATINGS\")\n    print(\"=\" * 70)\n    \n    # Prepare data\n    df_model = df_echo.dropna(subset=['accepted', 'avg_rating', 'ai_percentage'])\n    \n    # Create design matrix\n    X = df_model[['avg_rating', 'ai_percentage']].copy()\n    X = sm.add_constant(X)\n    y = df_model['accepted']\n    \n    print(f\"\\nSample size: {len(df_model):,}\")\n    \n    # Fit probit model\n    try:\n        probit_model = Probit(y, X)\n        probit_results = probit_model.fit(disp=0)\n        \n        print(\"\\n*** Probit Model: Acceptance ~ Rating + AI% ***\")\n        print(probit_results.summary2().tables[1].to_string())\n        \n        # Key coefficient: AI percentage\n        ai_coef = probit_results.params['ai_percentage']\n        ai_pval = probit_results.pvalues['ai_percentage']\n        ai_se = probit_results.bse['ai_percentage']\n        \n        print(f\"\\n*** KEY RESULT: AI Percentage Effect ***\")\n        print(f\"Coefficient: {ai_coef:.5f}\")\n        print(f\"Std Error: {ai_se:.5f}\")\n        print(f\"p-value: {ai_pval:.4e}\")\n        print(f\"Significant at 5%: {'YES' if ai_pval < 0.05 else 'NO'}\")\n        \n        # Interpretation\n        if ai_coef < 0 and ai_pval < 0.05:\n            print(\"\\n--> AI content NEGATIVELY affects acceptance beyond review scores\")\n        elif ai_coef > 0 and ai_pval < 0.05:\n            print(\"\\n--> AI content POSITIVELY affects acceptance beyond review scores\")\n        else:\n            print(\"\\n--> AI content effect is NOT significant conditional on scores\")\n            print(\"    (Any AI penalty operates through the score channel)\")\n            \n    except Exception as e:\n        print(f\"Model fitting failed: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Stratified analysis: Acceptance by rating bins for AI vs Human papers\nif 'accepted' in df_with_decisions.columns:\n    print(\"=\" * 70)\n    print(\"STRATIFIED ANALYSIS: ACCEPTANCE BY RATING BINS\")\n    print(\"=\" * 70)\n    \n    df_strat = df_clean.copy()\n    df_strat['rating_bin'] = pd.qcut(df_strat['avg_rating'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n    \n    # Acceptance by rating bin and paper type\n    strat_results = df_strat.groupby(['rating_bin', 'paper_type'])['accepted'].agg(['mean', 'count']).round(3)\n    print(\"\\nAcceptance Rate by Rating Bin and Paper Type:\")\n    print(strat_results.unstack())\n    \n    # Visualization\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Plot 1: Acceptance rate by AI percentage (continuous)\n    ax1 = axes[0]\n    df_plot = df_echo.copy()\n    df_plot['ai_decile'] = pd.qcut(df_plot['ai_percentage'], 10, labels=False, duplicates='drop')\n    acc_by_ai = df_plot.groupby('ai_decile').agg({\n        'ai_percentage': 'mean',\n        'accepted': ['mean', 'sem']\n    })\n    acc_by_ai.columns = ['ai_mean', 'acc_rate', 'acc_se']\n    \n    ax1.errorbar(acc_by_ai['ai_mean'], acc_by_ai['acc_rate'], \n                 yerr=1.96*acc_by_ai['acc_se'], fmt='o-', capsize=3, color='steelblue')\n    ax1.axhline(y=df_echo['accepted'].mean(), color='red', linestyle='--', alpha=0.5, label='Overall Rate')\n    ax1.set_xlabel('AI Percentage')\n    ax1.set_ylabel('Acceptance Rate')\n    ax1.set_title('ICLR 2026: Acceptance Rate by AI Content Level')\n    ax1.legend()\n    \n    # Plot 2: Acceptance rate by paper type within rating strata\n    ax2 = axes[1]\n    pivot = df_strat.pivot_table(values='accepted', index='rating_bin', columns='paper_type', aggfunc='mean')\n    pivot.plot(kind='bar', ax=ax2, color=['steelblue', 'coral'], edgecolor='black')\n    ax2.set_xlabel('Rating Bin')\n    ax2.set_ylabel('Acceptance Rate')\n    ax2.set_title('ICLR 2026: Acceptance by Rating Bin - AI vs Human Papers')\n    ax2.tick_params(axis='x', rotation=45)\n    ax2.legend(title='Paper Type')\n    \n    plt.tight_layout()\n    plt.savefig('last_results/iclr2026_acceptance_by_ai_content.png', dpi=150, bbox_inches='tight')\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. ICLR 2026 Interpretation and Conclusions\n\n### Key Findings:\n\n1. **AI ‚Üí Acceptance | Scores**: [Result will appear after running]\n   - If significant negative: Area chairs penalize AI content beyond reviewer scores\n   - If not significant: Any AI penalty operates through the score channel\n\n2. **Threshold Discontinuity**: [Result will appear after running]\n   - If significant: AI content is used as a tiebreaker at the margin\n   - If not significant: AI content doesn't affect marginal decisions\n\n3. **Presentation Tier**: [Result will appear after running]\n   - If significant negative: High-AI papers get relegated to poster tier\n   - Conditional on acceptance, so interpretation differs from rejection\n\n4. **Reputation √ó AI**: [Result will appear after running]\n   - If significant positive: Senior authors are protected from AI penalty\n   - Selection bias caveat applies\n\n5. **Echo Chamber (AI Reviews)**: [Result will appear after running]\n   - Positive interaction: AI reviewers favor AI papers\n   - Negative interaction: AI reviewers penalize AI papers\n   - This tests whether the review process itself is contaminated",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save enriched ICLR 2026 data with acceptance info\nif 'accepted' in df_with_decisions.columns:\n    # Save to repo directory (for git tracking)\n    repo_output_path = f'data/iclr{ICLR_YEAR}_submissions_with_acceptance.csv'\n    df_with_decisions.to_csv(repo_output_path, index=False)\n    print(f\"Saved to repo: {repo_output_path}\")\n    \n    # Also save to Google Drive (for persistence across sessions)\n    gdrive_output_path = f'{DATA_PATH}/iclr{ICLR_YEAR}_submissions_with_acceptance.csv'\n    os.makedirs(os.path.dirname(gdrive_output_path), exist_ok=True)\n    df_with_decisions.to_csv(gdrive_output_path, index=False)\n    print(f\"Saved to Google Drive: {gdrive_output_path}\")\n    \n    # Summary statistics\n    print(f\"\\n{'='*60}\")\n    print(f\"ICLR {ICLR_YEAR} FINAL SUMMARY\")\n    print(f\"{'='*60}\")\n    print(f\"Total submissions: {len(df_with_decisions):,}\")\n    print(f\"Accepted: {df_with_decisions['accepted'].sum():,} ({df_with_decisions['accepted'].mean():.1%})\")\n    print(f\"Mean AI percentage: {df_with_decisions['ai_percentage'].mean():.1f}%\")\n    print(f\"Mean avg_rating: {df_with_decisions['avg_rating'].mean():.2f}\")\n    \n    if 'tier_name' in df_with_decisions.columns:\n        print(f\"\\nPresentation Tier Distribution:\")\n        print(df_with_decisions[df_with_decisions['accepted']==1]['tier_name'].value_counts())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Acceptance by AI content and rating\n",
    "if 'accepted' in df_with_decisions.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Heatmap of acceptance rate by rating √ó AI content\n",
    "    ax1 = axes[0, 0]\n",
    "    df_temp = df_with_decisions.copy()\n",
    "    df_temp['rating_bin'] = pd.cut(df_temp['avg_rating'], bins=5)\n",
    "    df_temp['ai_bin'] = pd.cut(df_temp['ai_percentage'], bins=5)\n",
    "    pivot = df_temp.pivot_table(values='accepted', index='ai_bin', columns='rating_bin', aggfunc='mean')\n",
    "    sns.heatmap(pivot, annot=True, fmt='.2f', cmap='RdYlGn', ax=ax1)\n",
    "    ax1.set_title('Acceptance Rate by Rating and AI Content')\n",
    "    ax1.set_xlabel('Average Rating')\n",
    "    ax1.set_ylabel('AI Percentage')\n",
    "    \n",
    "    # 2. Acceptance rate vs AI percentage (binned)\n",
    "    ax2 = axes[0, 1]\n",
    "    df_temp['ai_decile'] = pd.qcut(df_temp['ai_percentage'], 10, labels=False, duplicates='drop')\n",
    "    acc_by_ai = df_temp.groupby('ai_decile').agg({\n",
    "        'ai_percentage': 'mean',\n",
    "        'accepted': ['mean', 'sem', 'count']\n",
    "    })\n",
    "    acc_by_ai.columns = ['ai_mean', 'acc_rate', 'acc_se', 'n']\n",
    "    \n",
    "    ax2.errorbar(acc_by_ai['ai_mean'], acc_by_ai['acc_rate'], \n",
    "                 yerr=1.96*acc_by_ai['acc_se'], fmt='o-', capsize=3)\n",
    "    ax2.set_xlabel('AI Percentage')\n",
    "    ax2.set_ylabel('Acceptance Rate')\n",
    "    ax2.set_title('Acceptance Rate by AI Content')\n",
    "    ax2.axhline(y=df_with_decisions['accepted'].mean(), color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 3. Regression discontinuity plot\n",
    "    ax3 = axes[1, 0]\n",
    "    margin_df = df_with_decisions[\n",
    "        (df_with_decisions['avg_rating'] >= 4.5) & \n",
    "        (df_with_decisions['avg_rating'] <= 5.5)\n",
    "    ]\n",
    "    ai_median = margin_df['ai_percentage'].median()\n",
    "    \n",
    "    high_ai = margin_df[margin_df['ai_percentage'] >= ai_median]\n",
    "    low_ai = margin_df[margin_df['ai_percentage'] < ai_median]\n",
    "    \n",
    "    for label, subset, color in [('High AI', high_ai, 'red'), ('Low AI', low_ai, 'green')]:\n",
    "        subset_sorted = subset.sort_values('avg_rating')\n",
    "        rolling_acc = subset_sorted['accepted'].rolling(20, min_periods=10).mean()\n",
    "        ax3.plot(subset_sorted['avg_rating'], rolling_acc, label=label, color=color, alpha=0.7)\n",
    "    \n",
    "    ax3.axvline(x=5.0, color='black', linestyle='--', alpha=0.5, label='Threshold')\n",
    "    ax3.set_xlabel('Average Rating')\n",
    "    ax3.set_ylabel('Acceptance Rate (Rolling Mean)')\n",
    "    ax3.set_title('Acceptance Near Margin by AI Content')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Tier distribution by AI content (if available)\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'tier_name' in df_with_decisions.columns:\n",
    "        accepted = df_with_decisions[df_with_decisions['accepted'] == 1]\n",
    "        accepted['ai_category'] = pd.cut(accepted['ai_percentage'], \n",
    "                                         bins=[0, 25, 75, 100], \n",
    "                                         labels=['Low AI', 'Medium AI', 'High AI'])\n",
    "        tier_by_ai = pd.crosstab(accepted['ai_category'], accepted['tier_name'], normalize='index')\n",
    "        tier_by_ai[['Poster', 'Spotlight', 'Oral']].plot(kind='bar', stacked=True, ax=ax4, \n",
    "                                                         color=['steelblue', 'orange', 'green'])\n",
    "        ax4.set_xlabel('AI Content Category')\n",
    "        ax4.set_ylabel('Proportion')\n",
    "        ax4.set_title('Presentation Tier by AI Content')\n",
    "        ax4.tick_params(axis='x', rotation=0)\n",
    "        ax4.legend(title='Tier')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Tier data not available', ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Presentation Tier by AI Content')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('last_results/acceptance_analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table of all tests\n",
    "if 'accepted' in df_with_decisions.columns:\n",
    "    summary_rows = []\n",
    "    \n",
    "    # Test 1: Probit\n",
    "    if 'ai_acceptance_probit' in results and results['ai_acceptance_probit']:\n",
    "        probit = results['ai_acceptance_probit'].get('probit', {})\n",
    "        if probit:\n",
    "            summary_rows.append({\n",
    "                'Test': 'AI ‚Üí Acceptance (Probit)',\n",
    "                'Coefficient': probit['params'].get('ai_percentage', np.nan),\n",
    "                'SE': probit['se'].get('ai_percentage', np.nan),\n",
    "                'p-value': probit['p_values'].get('ai_percentage', np.nan),\n",
    "                'Significant': probit['p_values'].get('ai_percentage', 1) < 0.05,\n",
    "                'N': results['ai_acceptance_probit']['n']\n",
    "            })\n",
    "    \n",
    "    # Test 2: Threshold Discontinuity\n",
    "    if 'threshold_discontinuity' in results and results['threshold_discontinuity']:\n",
    "        disc = results['threshold_discontinuity']\n",
    "        summary_rows.append({\n",
    "            'Test': 'Threshold Discontinuity',\n",
    "            'Coefficient': disc['acceptance_difference'],\n",
    "            'SE': np.nan,\n",
    "            'p-value': disc.get('chi2', {}).get('p_value', np.nan),\n",
    "            'Significant': disc.get('chi2', {}).get('p_value', 1) < 0.05,\n",
    "            'N': disc['n_margin']\n",
    "        })\n",
    "    \n",
    "    # Test 3: Presentation Tier\n",
    "    if 'presentation_tier' in results and results['presentation_tier']:\n",
    "        tier = results['presentation_tier'].get('ordered_probit', {})\n",
    "        if tier:\n",
    "            summary_rows.append({\n",
    "                'Test': 'Presentation Tier (Ord. Probit)',\n",
    "                'Coefficient': tier['params'].get('ai_percentage', np.nan),\n",
    "                'SE': tier['se'].get('ai_percentage', np.nan),\n",
    "                'p-value': tier['p_values'].get('ai_percentage', np.nan),\n",
    "                'Significant': tier['p_values'].get('ai_percentage', 1) < 0.05,\n",
    "                'N': results['presentation_tier']['n']\n",
    "            })\n",
    "    \n",
    "    # Test 4: Reputation Interaction\n",
    "    if 'reputation_interaction' in results and results['reputation_interaction']:\n",
    "        rep = results['reputation_interaction'].get('interaction_model', {})\n",
    "        if rep:\n",
    "            summary_rows.append({\n",
    "                'Test': 'Reputation √ó AI Interaction',\n",
    "                'Coefficient': rep['params'].get('ai_x_reputation', np.nan),\n",
    "                'SE': rep['se'].get('ai_x_reputation', np.nan),\n",
    "                'p-value': rep['p_values'].get('ai_x_reputation', np.nan),\n",
    "                'Significant': rep['p_values'].get('ai_x_reputation', 1) < 0.05,\n",
    "                'N': results['reputation_interaction']['n']\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY OF ACCEPTANCE ANALYSIS TESTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Save\n",
    "    summary_df.to_csv('last_results/acceptance_analysis_summary.csv', index=False)\n",
    "    print(\"\\nSaved to: last_results/acceptance_analysis_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}